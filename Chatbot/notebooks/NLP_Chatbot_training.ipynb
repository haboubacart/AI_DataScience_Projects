{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Chatbot_training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3BiHFqX_her","executionInfo":{"status":"ok","timestamp":1638377811253,"user_tz":0,"elapsed":809,"user":{"displayName":"Alioune Mbaye","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713240593601981120"}},"outputId":"cfcdf86a-225d-43de-a5e9-e90e15ccf0e7"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"p9_TAiK0-5Tk"},"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","import json\n","import pickle\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","from tensorflow.keras.optimizers import SGD\n","import random\n","words=[]\n","classes = []\n","documents = []\n","ignore_words = ['?', '!']\n","data_file = open('/content/drive/Shareddrives/Projets ING3 IA 2021-2022/Natural Language Processing/order.json').read()\n","intents = json.loads(data_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfImOmtVABvz","executionInfo":{"status":"ok","timestamp":1638377817165,"user_tz":0,"elapsed":327,"user":{"displayName":"Alioune Mbaye","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713240593601981120"}},"outputId":"a6dbbbe9-2156-4a15-8147-4609c8996e57"},"source":["nltk.download('punkt')\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","        #tokenize each word\n","        w = nltk.word_tokenize(pattern)\n","        words.extend(w)\n","        #add documents in the corpus\n","        documents.append((w, intent['tag']))\n","        # add to our classes list\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIhvXlWhALMe","executionInfo":{"status":"ok","timestamp":1638377819794,"user_tz":0,"elapsed":327,"user":{"displayName":"Alioune Mbaye","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713240593601981120"}},"outputId":"4ca4ae7e-fe9f-4e9d-a53f-063fe660e4e7"},"source":["nltk.download('wordnet')\n","# lemmatize, lower each word and remove duplicates\n","words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n","words = sorted(list(set(words)))\n","# sort classes\n","classes = sorted(list(set(classes)))\n","# documents = combination between patterns and intents\n","print (len(documents), \"documents\")\n","# classes = intents\n","print (len(classes), \"classes\", classes)\n","# words = all words, vocabulary\n","print (len(words), \"unique lemmatized words\", words)\n","\n","pickle.dump(words,open('data/words.pkl','wb'))\n","pickle.dump(classes,open('data/classes.pkl','wb'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","181 documents\n","58 classes ['God', 'HR_related_problem', 'Location', 'Weather', 'about', 'about me', 'alive', 'appointment status', 'bookings', 'cabin', 'chatbot', 'check_leave', 'commission', 'competitors_in_market', 'configuration', 'connect_people', 'cost_lowering', 'creator', 'customer_satisfaction', 'domain', 'email_id', 'factors_impacting_sale', 'forgot_password', 'gadgets', 'goodbye', 'google', 'greeting', 'highest_grossing', 'hours', 'invalid', 'joke', 'key_customers', 'killing', 'leave', 'maintainence', 'manufacturing_problems', 'missing_id', 'myself', 'name', 'news', 'noans', 'options', 'order_components', 'order_tracking', 'predict_delay', 'predict_performance', 'project_handling_queries', 'search_department', 'search_person_by_id', 'solve_problems', 'stories', 'supplier_info', 'talk', 'thanks', 'turnover', 'version_update', 'weather', 'wikipedia']\n","286 unique lemmatized words [\"'s\", ',', '.', '23a31', '32712', '345a23', '431b67', '561a24', '562b78', '@', 'a', 'about', 'accident', 'ai', 'ali', 'alioune', 'alive', 'all', 'am', 'an', 'analysis', 'and', 'anyone', 'anything', 'appointment', 'appoitment', 'are', 'at', 'awesome', 'based', 'be', 'been', 'benefit', 'book', 'bored', 'breathe', 'bye', 'cab', 'cabin', 'cafeteria', 'call', 'can', 'canteen', 'challenging', 'change', 'chat', 'chatting', 'clarity', 'clear', 'commission', 'company', 'compensation', 'complaint', 'component', 'comprises', 'computer', 'configuration', 'configure', 'conflict', 'cost', 'could', 'created', 'creator', 'current', 'customer', 'cy-tech.fr', 'cybot', 'daddy', 'date', 'day', 'delayed', 'delivery', 'demand', 'department', 'design', 'desktop', 'detail', 'development', 'do', 'doe', 'doing', 'domain', 'each', 'earth', 'employee', 'event', 'everyone', 'exists', 'factor', 'father', 'feedback', 'find', 'fixed', 'for', 'forgets', 'forgot', 'from', 'funny', 'gadget', 'get', 'gmail.com', 'god', 'good', 'goodbye', 'google', 'googling', 'grossing', 'guide', 'ha', 'haboubacar', 'handled', 'happy', 'have', 'head', 'headline', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'highest', 'hola', 'hotel', 'hour', 'how', 'hr/it/projects', 'i', 'id', 'identify', 'impact', 'impacting', 'improve', 'improved', 'improving', 'in', 'information', 'insufficient', 'is', 'issue', 'it', 'job', 'joke', 'kera', 'key', 'kill', 'know', 'knowledge', 'lack', 'laptop', 'last', 'later', 'laugh', 'leave', 'legal', 'like', 'list', 'locate', 'location', 'login', 'love', 'lower', 'lowering', \"m'baye\", 'made', 'maintainence', 'make', 'management', 'manufacturer', 'market', 'marry', 'mbayealiou', 'me', 'meet', 'member', 'miscommunication', 'murder', 'my', 'myself', 'name', 'nearby', 'need', 'needed', 'news', 'next', 'nice', 'not', 'occured', 'of', 'office', 'on', 'open', 'opening', 'order', 'our', 'password', 'planning', 'present', 'problem', 'product', 'profile', 'profit', 'project', 'provide', 'query', 'raised', 'rate', 'recorded', 'related', 'reservation', 'resolved', 'response', 'restraunts', 'risk', 'run', 'ryan', 'sale', 'science', 'search', 'see', 'set', 'shared', 'should', 'sing', 'skill', 'skilled', 'software', 'someone', 'something', 'specification', 'step', 'stock', 'story', 'supplier', 'table', 'talk', 'target', 'team', 'tell', 'thank', 'thanks', 'that', 'the', 'there', 'this', 'threat', 'ticket', 'tidjani', 'till', 'time', 'to', 'today', 'top', 'track', 'training', 'turnover', 'u', 'update', 'updated', 'updation', 'urgently', 'user', 'various', 'version', 'vp', 'wa', 'want', 'we', 'weather', 'what', 'when', 'where', 'which', 'who', 'why', 'wifi', 'wiki', 'wikipedia', 'with', 'workforce', 'working', 'yannis', 'year', 'you', 'your', 'yourself']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUd684tPAYrR","executionInfo":{"status":"ok","timestamp":1638377824457,"user_tz":0,"elapsed":268,"user":{"displayName":"Alioune Mbaye","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713240593601981120"}},"outputId":"81792517-15ba-4ac5-a371-74c1d53c4131"},"source":["# create our training data\n","training = []\n","# create an empty array for our output\n","output_empty = [0] * len(classes)\n","# training set, bag of words for each sentence\n","for doc in documents:\n","    # initialize our bag of words\n","    bag = []\n","    # list of tokenized words for the pattern\n","    pattern_words = doc[0]\n","    # lemmatize each word - create base word, in attempt to represent related words\n","    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n","    # create our bag of words array with 1, if word match found in current pattern\n","    for w in words:\n","        bag.append(1) if w in pattern_words else bag.append(0)\n","    # output is a '0' for each tag and '1' for current tag (for each pattern)\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","    training.append([bag, output_row])\n","# shuffle our features and turn into np.array\n","random.shuffle(training)\n","training = np.array(training)\n","# create train and test lists. X - patterns, Y - intents\n","train_x = list(training[:,0])\n","train_y = list(training[:,1])\n","print(\"Training data created\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data created\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"]}]},{"cell_type":"code","metadata":{"id":"o9Vnsu2tIsR6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638287865907,"user_tz":0,"elapsed":411,"user":{"displayName":"Alioune Mbaye","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713240593601981120"}},"outputId":"41f43053-57a8-433b-8e7c-444db5346d50"},"source":["predict_class(\"Hi\",model)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'intent': 'greeting', 'probability': '0.92756826'}]"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"K2sjocBFAo3i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638377845785,"user_tz":0,"elapsed":17539,"user":{"displayName":"Alioune Mbaye","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16713240593601981120"}},"outputId":"3056cc02-fd56-405b-cbd6-c3a1b4fcdaa5"},"source":["model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation='softmax'))\n","sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n","model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n","hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n","model.save('data/chatbot_model.h5', hist)\n","print(\"model created\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","37/37 [==============================] - 1s 2ms/step - loss: 4.0702 - accuracy: 0.0221\n","Epoch 2/200\n","37/37 [==============================] - 0s 2ms/step - loss: 4.0149 - accuracy: 0.0331\n","Epoch 3/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.9500 - accuracy: 0.0718\n","Epoch 4/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.8778 - accuracy: 0.0829\n","Epoch 5/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.7906 - accuracy: 0.0994\n","Epoch 6/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.6447 - accuracy: 0.0939\n","Epoch 7/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.6118 - accuracy: 0.1215\n","Epoch 8/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.4947 - accuracy: 0.1657\n","Epoch 9/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.3158 - accuracy: 0.1989\n","Epoch 10/200\n","37/37 [==============================] - 0s 2ms/step - loss: 3.1807 - accuracy: 0.2099\n","Epoch 11/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.9861 - accuracy: 0.2762\n","Epoch 12/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.9239 - accuracy: 0.2818\n","Epoch 13/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.7478 - accuracy: 0.2707\n","Epoch 14/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.5358 - accuracy: 0.3536\n","Epoch 15/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.5002 - accuracy: 0.3481\n","Epoch 16/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.4436 - accuracy: 0.3702\n","Epoch 17/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.2268 - accuracy: 0.4586\n","Epoch 18/200\n","37/37 [==============================] - 0s 2ms/step - loss: 2.2330 - accuracy: 0.4088\n","Epoch 19/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.9159 - accuracy: 0.4917\n","Epoch 20/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.9926 - accuracy: 0.4696\n","Epoch 21/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.7451 - accuracy: 0.5304\n","Epoch 22/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.6078 - accuracy: 0.5304\n","Epoch 23/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.6183 - accuracy: 0.5193\n","Epoch 24/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.6194 - accuracy: 0.5525\n","Epoch 25/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.5662 - accuracy: 0.5912\n","Epoch 26/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.4727 - accuracy: 0.6022\n","Epoch 27/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.4180 - accuracy: 0.5967\n","Epoch 28/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.3036 - accuracy: 0.6243\n","Epoch 29/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.0900 - accuracy: 0.6519\n","Epoch 30/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.1627 - accuracy: 0.6630\n","Epoch 31/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.0898 - accuracy: 0.7072\n","Epoch 32/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.1407 - accuracy: 0.6851\n","Epoch 33/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.1306 - accuracy: 0.6685\n","Epoch 34/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.0498 - accuracy: 0.6961\n","Epoch 35/200\n","37/37 [==============================] - 0s 2ms/step - loss: 1.0762 - accuracy: 0.6906\n","Epoch 36/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.9226 - accuracy: 0.6906\n","Epoch 37/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.9066 - accuracy: 0.7459\n","Epoch 38/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.8740 - accuracy: 0.7845\n","Epoch 39/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.9856 - accuracy: 0.6851\n","Epoch 40/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.8337 - accuracy: 0.7569\n","Epoch 41/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.8152 - accuracy: 0.7569\n","Epoch 42/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.8121 - accuracy: 0.7403\n","Epoch 43/200\n","37/37 [==============================] - 0s 3ms/step - loss: 0.9601 - accuracy: 0.7127\n","Epoch 44/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.8213 - accuracy: 0.7182\n","Epoch 45/200\n","37/37 [==============================] - 0s 3ms/step - loss: 0.8873 - accuracy: 0.7293\n","Epoch 46/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.8443 - accuracy: 0.7459\n","Epoch 47/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.7274 - accuracy: 0.7680\n","Epoch 48/200\n","37/37 [==============================] - 0s 3ms/step - loss: 0.7618 - accuracy: 0.7845\n","Epoch 49/200\n","37/37 [==============================] - 0s 3ms/step - loss: 0.8481 - accuracy: 0.7680\n","Epoch 50/200\n","37/37 [==============================] - 0s 3ms/step - loss: 0.8117 - accuracy: 0.7459\n","Epoch 51/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.7774 - accuracy: 0.7680\n","Epoch 52/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.7113 - accuracy: 0.7735\n","Epoch 53/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6768 - accuracy: 0.7735\n","Epoch 54/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.7062 - accuracy: 0.7845\n","Epoch 55/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.8122\n","Epoch 56/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.7036 - accuracy: 0.7790\n","Epoch 57/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.7680\n","Epoch 58/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.7007 - accuracy: 0.7845\n","Epoch 59/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.8011\n","Epoch 60/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6364 - accuracy: 0.8011\n","Epoch 61/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6904 - accuracy: 0.7845\n","Epoch 62/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.8066\n","Epoch 63/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6746 - accuracy: 0.7514\n","Epoch 64/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8619\n","Epoch 65/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6014 - accuracy: 0.8177\n","Epoch 66/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5454 - accuracy: 0.8177\n","Epoch 67/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.7234 - accuracy: 0.8066\n","Epoch 68/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.8122\n","Epoch 69/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.8122\n","Epoch 70/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.8619\n","Epoch 71/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.8177\n","Epoch 72/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6141 - accuracy: 0.8177\n","Epoch 73/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4616 - accuracy: 0.8564\n","Epoch 74/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.8453\n","Epoch 75/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.8177\n","Epoch 76/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6095 - accuracy: 0.8177\n","Epoch 77/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.8674\n","Epoch 78/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6131 - accuracy: 0.7901\n","Epoch 79/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5739 - accuracy: 0.7956\n","Epoch 80/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.8343\n","Epoch 81/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.8066\n","Epoch 82/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8674\n","Epoch 83/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8564\n","Epoch 84/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5147 - accuracy: 0.8453\n","Epoch 85/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.8508\n","Epoch 86/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.8619\n","Epoch 87/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5475 - accuracy: 0.8232\n","Epoch 88/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.8287\n","Epoch 89/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.7680\n","Epoch 90/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.8177\n","Epoch 91/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8785\n","Epoch 92/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.8232\n","Epoch 93/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8729\n","Epoch 94/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.8564\n","Epoch 95/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8564\n","Epoch 96/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.8840\n","Epoch 97/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8619\n","Epoch 98/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8619\n","Epoch 99/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8729\n","Epoch 100/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8619\n","Epoch 101/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8840\n","Epoch 102/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.8177\n","Epoch 103/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8729\n","Epoch 104/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.8619\n","Epoch 105/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.8840\n","Epoch 106/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.8508\n","Epoch 107/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8895\n","Epoch 108/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.8564\n","Epoch 109/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5977 - accuracy: 0.8066\n","Epoch 110/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8674\n","Epoch 111/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8508\n","Epoch 112/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4237 - accuracy: 0.8729\n","Epoch 113/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8895\n","Epoch 114/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.8177\n","Epoch 115/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8619\n","Epoch 116/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8453\n","Epoch 117/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8564\n","Epoch 118/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8729\n","Epoch 119/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.8453\n","Epoch 120/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.8729\n","Epoch 121/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.8729\n","Epoch 122/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8619\n","Epoch 123/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8895\n","Epoch 124/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8564\n","Epoch 125/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8895\n","Epoch 126/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8564\n","Epoch 127/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8950\n","Epoch 128/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.9116\n","Epoch 129/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.9171\n","Epoch 130/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8564\n","Epoch 131/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.8343\n","Epoch 132/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8674\n","Epoch 133/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.9006\n","Epoch 134/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.8564\n","Epoch 135/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8950\n","Epoch 136/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3246 - accuracy: 0.8950\n","Epoch 137/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8785\n","Epoch 138/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3560 - accuracy: 0.8950\n","Epoch 139/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8674\n","Epoch 140/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3548 - accuracy: 0.8729\n","Epoch 141/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8785\n","Epoch 142/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8729\n","Epoch 143/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8453\n","Epoch 144/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.9006\n","Epoch 145/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3574 - accuracy: 0.8785\n","Epoch 146/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8674\n","Epoch 147/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8508\n","Epoch 148/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.8785\n","Epoch 149/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.9116\n","Epoch 150/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8895\n","Epoch 151/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.8895\n","Epoch 152/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8950\n","Epoch 153/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8729\n","Epoch 154/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8785\n","Epoch 155/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.9006\n","Epoch 156/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8895\n","Epoch 157/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2556 - accuracy: 0.9282\n","Epoch 158/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.8564\n","Epoch 159/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8674\n","Epoch 160/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.8453\n","Epoch 161/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2671 - accuracy: 0.9116\n","Epoch 162/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8950\n","Epoch 163/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2786 - accuracy: 0.8895\n","Epoch 164/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.8895\n","Epoch 165/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.8398\n","Epoch 166/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3146 - accuracy: 0.8674\n","Epoch 167/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.9006\n","Epoch 168/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8785\n","Epoch 169/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2833 - accuracy: 0.9171\n","Epoch 170/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8785\n","Epoch 171/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3006 - accuracy: 0.9006\n","Epoch 172/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3534 - accuracy: 0.8729\n","Epoch 173/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.9006\n","Epoch 174/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2635 - accuracy: 0.9061\n","Epoch 175/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3506 - accuracy: 0.8895\n","Epoch 176/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8895\n","Epoch 177/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.9061\n","Epoch 178/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.9171\n","Epoch 179/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8785\n","Epoch 180/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.5260 - accuracy: 0.8398\n","Epoch 181/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.8619\n","Epoch 182/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3453 - accuracy: 0.8840\n","Epoch 183/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.9061\n","Epoch 184/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8950\n","Epoch 185/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.9116\n","Epoch 186/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3215 - accuracy: 0.8785\n","Epoch 187/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2519 - accuracy: 0.9171\n","Epoch 188/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.9061\n","Epoch 189/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3310 - accuracy: 0.8895\n","Epoch 190/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8564\n","Epoch 191/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8950\n","Epoch 192/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8840\n","Epoch 193/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.9006\n","Epoch 194/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.9116\n","Epoch 195/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.9116\n","Epoch 196/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3174 - accuracy: 0.9061\n","Epoch 197/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.8398\n","Epoch 198/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8840\n","Epoch 199/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2194 - accuracy: 0.9337\n","Epoch 200/200\n","37/37 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.9006\n","model created\n"]}]},{"cell_type":"code","metadata":{"id":"N0vmM3cXBb43"},"source":["def clean_up_sentence(sentence):\n","    # tokenize the pattern - split words into array\n","    sentence_words = nltk.word_tokenize(sentence)\n","    # stem each word - create short form for word\n","    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n","    return sentence_words\n","# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n","def bow(sentence, words, show_details=True):\n","    # tokenize the pattern\n","    sentence_words = clean_up_sentence(sentence)\n","    # bag of words - matrix of N words, vocabulary matrix\n","    bag = [0]*len(words) \n","    for s in sentence_words:\n","        for i,w in enumerate(words):\n","            if w == s: \n","                # assign 1 if current word is in the vocabulary position\n","                bag[i] = 1\n","                if show_details:\n","                    print (\"found in bag: %s\" % w)\n","    return(np.array(bag))\n","def predict_class(sentence, model):\n","    # filter out predictions below a threshold\n","    p = bow(sentence, words,show_details=False)\n","    res = model.predict(np.array([p]))[0]\n","    ERROR_THRESHOLD = 0.25\n","    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n","    # sort by strength of probability\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    return_list = []\n","    for r in results:\n","        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n","    return return_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lp-UBGX2Bwkv"},"source":["def getResponse(ints, intents_json):\n","    tag = ints[0]['intent']\n","    list_of_intents = intents_json['intents']\n","    for i in list_of_intents:\n","        if(i['tag']== tag):\n","            result = random.choice(i['responses'])\n","            break\n","    return result\n","\n","def chatbot_response(text):\n","    ints = predict_class(text, model)\n","    res = getResponse(ints, intents)\n","    return res"],"execution_count":null,"outputs":[]}]}